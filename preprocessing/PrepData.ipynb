{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "612382c8",
   "metadata": {},
   "source": [
    "### Lister tous les fichiers audio avec leurs labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4bb0478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "\n",
    "genres = sorted(os.listdir(\"../Data/genres_original\"))  \n",
    "\n",
    "\n",
    "filepaths = []\n",
    "labels = []\n",
    "\n",
    "for idx , genre in enumerate(genres):\n",
    "    files = glob.glob(f\"../Data/genres_original/{genre}/*.wav\")\n",
    "    for f in files :\n",
    "        filepaths.append(f)\n",
    "        labels.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44965f72",
   "metadata": {},
   "source": [
    "### Découper en train / val / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "355eb356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699 train\n",
      "151 val\n",
      "150 test\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    filepaths, labels, test_size=0.15, random_state=42, stratify=labels)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp)\n",
    " \n",
    "\n",
    "print(len(X_train), \"train\")\n",
    "print(len(X_val),   \"val\")\n",
    "print(len(X_test),  \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8720c6",
   "metadata": {},
   "source": [
    "### découper les fichiers en segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "72fc2517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import audioread  # force fallback\n",
    "\n",
    "def split_audio(file, segment_duration=3, sr=22050):\n",
    "    try:\n",
    "        signal, _ = librosa.load(file, sr=sr)\n",
    "    except Exception as e:\n",
    "        print(\"Erreur sur:\", file, e)\n",
    "        return []  # ignorer le fichier problématique\n",
    "\n",
    "    samples_per_segment = sr * segment_duration\n",
    "    segments = []\n",
    "\n",
    "    for start in range(0, len(signal), samples_per_segment):\n",
    "        end = start + samples_per_segment\n",
    "        part = signal[start:end]\n",
    "        if len(part) == samples_per_segment:\n",
    "            segments.append(part)\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e458af",
   "metadata": {},
   "source": [
    "### Appliquer la découpe à train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0436090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/r64x99yx2k91vgyysnwnb7pc0000gn/T/ipykernel_15537/2648669402.py:6: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  signal, _ = librosa.load(file, sr=sr)\n",
      "/Users/mac/Desktop/Ast_Projet/.venv/lib/python3.12/site-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erreur sur: ../Data/genres_original/jazz/jazz.00054.wav \n",
      "Train segments: 6977\n"
     ]
    }
   ],
   "source": [
    "X_train_segments = []\n",
    "y_train_segments = []\n",
    "\n",
    "for idx, file in enumerate(X_train):\n",
    "    segs = split_audio(file)\n",
    "    X_train_segments.extend(segs)\n",
    "    y_train_segments.extend([y_train[idx]] * len(segs))\n",
    "\n",
    "print(\"Train segments:\", len(X_train_segments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccce133",
   "metadata": {},
   "source": [
    "### Appliquer la découpe à test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6d22521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test segments: 1495\n"
     ]
    }
   ],
   "source": [
    "X_test_segments = []\n",
    "y_test_segments = []\n",
    "\n",
    "for idx, file in enumerate(X_test):\n",
    "    segs = split_audio(file)\n",
    "    X_test_segments.extend(segs)\n",
    "    y_test_segments.extend([y_test[idx]] * len(segs))\n",
    "\n",
    "print(\"Test segments:\", len(X_test_segments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca109475",
   "metadata": {},
   "source": [
    "### Appliquer la découpe à val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c981729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val segments: 1509\n"
     ]
    }
   ],
   "source": [
    "X_val_segments = []\n",
    "y_val_segments = []\n",
    "\n",
    "for idx, file in enumerate(X_val):\n",
    "    segs = split_audio(file)\n",
    "    X_val_segments.extend(segs)\n",
    "    y_val_segments.extend([y_val[idx]] * len(segs))\n",
    "\n",
    "print(\"Val segments:\", len(X_val_segments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777633b6",
   "metadata": {},
   "source": [
    "### convertir un segment → log-mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50928f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def segment_to_logmel(segment, sr=22050, n_mels=128, n_fft=1024, hop_length=512):\n",
    "    # Mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=segment,\n",
    "        sr=sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    \n",
    "    # Log-mel\n",
    "    log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    # Normalisation\n",
    "    log_mel_norm = (log_mel - log_mel.min()) / (log_mel.max() - log_mel.min())\n",
    "    \n",
    "    # Ajustement de la largeur à 128\n",
    "    log_mel_norm = librosa.util.fix_length(log_mel_norm, size=128, axis=1)\n",
    "    \n",
    "    return log_mel_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ddc8e",
   "metadata": {},
   "source": [
    "### convertir train, val, test en log mel spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58bc6094",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mel = [segment_to_logmel(seg) for seg in X_train_segments]\n",
    "X_val_mel  = [segment_to_logmel(seg) for seg in X_val_segments]\n",
    "X_test_mel = [segment_to_logmel(seg) for seg in X_test_segments]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6af6391",
   "metadata": {},
   "source": [
    "### Convertir en tableaux NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "777f5700",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mel = np.array(X_train_mel)\n",
    "X_val_mel   = np.array(X_val_mel)\n",
    "X_test_mel  = np.array(X_test_mel)\n",
    "\n",
    "y_train_segments = np.array(y_train_segments)\n",
    "y_val_segments   = np.array(y_val_segments)\n",
    "y_test_segments  = np.array(y_test_segments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145600b4",
   "metadata": {},
   "source": [
    "### Créer un Dataset PyTorch avec log-me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71a27e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GTZANDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X : numpy array des log-mels\n",
    "        # y : numpy array des labels\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # convertir en tensor float\n",
    "        self.y = torch.tensor(y, dtype=torch.long)     # labels int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71ca10",
   "metadata": {},
   "source": [
    "### Créer les Datasets et DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8ba6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = GTZANDataset(X_train_mel, y_train_segments)\n",
    "val_dataset   = GTZANDataset(X_val_mel, y_val_segments)\n",
    "test_dataset  = GTZANDataset(X_test_mel, y_test_segments)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6208cf3",
   "metadata": {},
   "source": [
    "### Importer le modèle et le charger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5197901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------AST Model Summary---------------\n",
      "ImageNet pretraining: False, AudioSet pretraining: False\n",
      "frequncey stride=10, time stride=10\n",
      "number of patches=144\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# chemin vers le dossier contenant ast_models.py\n",
    "sys.path.append(\"../../../ast/src/models\")\n",
    "\n",
    "from ast_models import ASTModel\n",
    "\n",
    "num_classes = 10  # pour GTZAN\n",
    "\n",
    "model = ASTModel(\n",
    "    label_dim=num_classes,     # change la head pour 10 classes\n",
    "    input_fdim=128,            # nombre de bins mel\n",
    "    input_tdim=128,             # nombre de frames temporelles de tes log‑mels\n",
    "    imagenet_pretrain=False,    # ou selon ce que tu veux\n",
    "    audioset_pretrain=False    # si tu veux partir de pré‑entraînement AudioSet ou pas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af0fa6",
   "metadata": {},
   "source": [
    "### Définir loss et optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e5e7930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa243718",
   "metadata": {},
   "source": [
    "### Boucle d’entraînement simplifiée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a948b3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.1887\n",
      "Epoch 2/10, Loss: 1.7320\n",
      "Epoch 3/10, Loss: 1.5603\n",
      "Epoch 4/10, Loss: 1.3306\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m outputs = model(X_batch)\n\u001b[32m     10\u001b[39m loss = criterion(outputs, y_batch)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m optimizer.step()\n\u001b[32m     13\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ast_Projet/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ast_Projet/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Ast_Projet/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
